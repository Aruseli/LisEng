import dotenv from 'dotenv';
import { Hasura } from 'hasyx/lib/hasura/hasura';

dotenv.config();

/**
 * –ú–∏–≥—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ vocabulary_cards
 * 
 * –õ–æ–≥–∏–∫–∞:
 * 1. –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Å–ª–æ–≤—É (LOWER(TRIM(word))) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
 * 2. –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤—ã–±–∏—Ä–∞–µ—Ç —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∫–∞—Ä—Ç–æ—á–∫—É –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
 * 3. –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ example_sentence —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å " | "
 * 4. –û–±–Ω–æ–≤–ª—è–µ—Ç –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (active_recall_sessions, review_history) –Ω–∞ ID –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏
 * 5. –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã
 */
export default async function up() {
  const hasura = new Hasura({
    url: process.env.NEXT_PUBLIC_HASURA_GRAPHQL_URL!,
    secret: process.env.HASURA_ADMIN_SECRET!,
  });

  await hasura.sql('BEGIN');

  try {
    console.log('üìù Starting vocabulary cards deduplication...');

    // –®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    await hasura.sql(`
      CREATE TEMP TABLE IF NOT EXISTS duplicate_groups AS
      SELECT 
        user_id,
        LOWER(TRIM(word)) AS normalized_word,
        MIN(created_at) AS oldest_created_at,
        MIN(added_date) AS oldest_added_date
      FROM vocabulary_cards
      GROUP BY user_id, LOWER(TRIM(word))
      HAVING COUNT(*) > 1;
    `);

    console.log('‚úÖ Created duplicate groups table');

    // –®–∞–≥ 2: –ù–∞—Ö–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ä—Ç–æ—á–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (—Å–∞–º–∞—è —Å—Ç–∞—Ä–∞—è)
    await hasura.sql(`
      CREATE TEMP TABLE IF NOT EXISTS primary_cards AS
      SELECT DISTINCT ON (dg.user_id, dg.normalized_word)
        vc.id AS primary_id,
        dg.user_id,
        dg.normalized_word
      FROM duplicate_groups dg
      INNER JOIN vocabulary_cards vc 
        ON vc.user_id = dg.user_id 
        AND LOWER(TRIM(vc.word)) = dg.normalized_word
      ORDER BY 
        dg.user_id, 
        dg.normalized_word,
        COALESCE(vc.added_date, vc.created_at::date) ASC,
        vc.created_at ASC
      LIMIT 1;
    `);

    console.log('‚úÖ Identified primary cards');

    // –®–∞–≥ 3: –û–±—ä–µ–¥–∏–Ω—è–µ–º example_sentence –¥–ª—è –≤—Å–µ—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–µ
    await hasura.sql(`
      UPDATE vocabulary_cards vc
      SET example_sentence = (
        SELECT 
          STRING_AGG(
            DISTINCT COALESCE(example_sentence, ''),
            ' | '
            ORDER BY COALESCE(example_sentence, '')
          )
        FROM vocabulary_cards vc2
        WHERE vc2.user_id = vc.user_id
          AND LOWER(TRIM(vc2.word)) = LOWER(TRIM(vc.word))
          AND COALESCE(vc2.example_sentence, '') != ''
      )
      WHERE EXISTS (
        SELECT 1 
        FROM primary_cards pc
        WHERE pc.primary_id = vc.id
      )
      AND (
        SELECT COUNT(*)
        FROM vocabulary_cards vc3
        WHERE vc3.user_id = vc.user_id
          AND LOWER(TRIM(vc3.word)) = LOWER(TRIM(vc.word))
      ) > 1;
    `);

    console.log('‚úÖ Merged example sentences');

    // –®–∞–≥ 4: –û–±–Ω–æ–≤–ª—è–µ–º active_recall_sessions - –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ä—Ç–æ—á–∫—É
    await hasura.sql(`
      UPDATE active_recall_sessions ars
      SET recall_item_id = pc.primary_id
      FROM primary_cards pc, vocabulary_cards vc
      WHERE vc.id = ars.recall_item_id
        AND ars.recall_item_type = 'vocabulary_card'
        AND vc.user_id = pc.user_id
        AND LOWER(TRIM(vc.word)) = pc.normalized_word
        AND ars.recall_item_id != pc.primary_id;
    `);

    console.log('‚úÖ Updated active_recall_sessions references');

    // –®–∞–≥ 5: –û–±–Ω–æ–≤–ª—è–µ–º review_history - –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ä—Ç–æ—á–∫—É
    await hasura.sql(`
      UPDATE review_history rh
      SET card_id = pc.primary_id
      FROM primary_cards pc, vocabulary_cards vc
      WHERE vc.id = rh.card_id
        AND vc.user_id = pc.user_id
        AND LOWER(TRIM(vc.word)) = pc.normalized_word
        AND rh.card_id != pc.primary_id;
    `);

    console.log('‚úÖ Updated review_history references');

    // –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏)
    await hasura.sql(`
      DELETE FROM vocabulary_cards vc
      WHERE EXISTS (
        SELECT 1 
        FROM duplicate_groups dg
        WHERE dg.user_id = vc.user_id
          AND LOWER(TRIM(vc.word)) = dg.normalized_word
      )
      AND NOT EXISTS (
        SELECT 1 
        FROM primary_cards pc
        WHERE pc.primary_id = vc.id
      );
    `);

    console.log('‚úÖ Deleted duplicate cards');

    // –®–∞–≥ 7: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞ (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
    await hasura.sql(`
      UPDATE vocabulary_cards
      SET word = LOWER(TRIM(word))
      WHERE word != LOWER(TRIM(word));
    `);

    console.log('‚úÖ Normalized remaining words');

    await hasura.sql('COMMIT');
    console.log('‚úÖ Vocabulary cards deduplication completed successfully');
  } catch (error) {
    await hasura.sql('ROLLBACK');
    console.error('‚ùå Vocabulary cards deduplication failed:', error);
    throw error;
  }
}

up();

